import os
import json
import random
import numpy as np
from PIL import Image
from sklearn.model_selection import train_test_split
from tensorflow.keras.models import Sequential, load_model
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Input, Dropout
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, LambdaCallback, ReduceLROnPlateau
from tensorflow.keras.regularizers import l2
from tensorflow.keras.preprocessing.image import ImageDataGenerator
import tensorflow.keras as keras
from tensorflow.keras.optimizers.schedules import ExponentialDecay
from google.colab import drive

SAVE_DIR = '/content/drive/MyDrive/saved_model_results/'
os.makedirs(SAVE_DIR, exist_ok=True)
last_epoch_path = os.path.join(SAVE_DIR, 'last_epoch.txt')
EXTRACTED_PATH = "/content/sample_data/my_class/by_class"
DATASET_PATH = EXTRACTED_PATH

def custom_image_generator(dataset_path, batch_size, image_size=(28, 28), shuffle=True, seed=42):
    random.seed(seed)
    class_folders = [os.path.join(dataset_path, label) for label in os.listdir(dataset_path)]
    class_labels = {label: idx for idx, label in enumerate(os.listdir(dataset_path))}
    all_images = []
    for class_folder in class_folders:
        img_files = [
            os.path.join(root, file)
            for root, _, files in os.walk(class_folder)
            for file in files if file.endswith(".png")
        ]
        all_images.extend((img, class_labels[os.path.basename(class_folder)]) for img in img_files)
    while True:
        if shuffle:
            random.shuffle(all_images)
        for i in range(0, len(all_images), batch_size):
            batch_images, batch_labels = [], []
            batch_data = all_images[i:i + batch_size]
            for img_path, label in batch_data:
                img = Image.open(img_path).convert('L').resize(image_size)
                batch_images.append(np.array(img) / 255.0)
                batch_labels.append(label)
            yield np.expand_dims(np.array(batch_images), -1), to_categorical(batch_labels, num_classes=len(class_labels))

datagen = ImageDataGenerator(
    rotation_range=30,
    width_shift_range=0.2,
    height_shift_range=0.2,
    shear_range=0.2,
    zoom_range=0.2,
    horizontal_flip=False,
    fill_mode='nearest'
)

BATCH_SIZE = 32
train_generator = datagen.flow_from_directory(
    DATASET_PATH,
    target_size=(28, 28),
    batch_size=BATCH_SIZE,
    class_mode='categorical',
    color_mode='grayscale'
)
validation_generator = custom_image_generator(DATASET_PATH, batch_size=BATCH_SIZE)

model = Sequential([
    Input(shape=(28, 28, 1)),
    Conv2D(32, (3, 3), activation='relu', kernel_regularizer=l2(0.001)),
    MaxPooling2D((2, 2)),
    Conv2D(64, (3, 3), activation='relu', kernel_regularizer=l2(0.001)),
    MaxPooling2D((2, 2)),
    Flatten(),
    Dropout(0.7),
    Dense(128, activation='relu'),
    Dense(len(os.listdir(DATASET_PATH)), activation='softmax')
])

model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

checkpoint_path = os.path.join(SAVE_DIR, 'latest_checkpoint.keras')
if os.path.exists(checkpoint_path):
    print(f"Checkpoint found. Loading model from {checkpoint_path}")
    model = load_model(checkpoint_path)
else:
    print("No checkpoint found. Starting training from scratch.")

if os.path.exists(last_epoch_path):
    with open(last_epoch_path, 'r') as f:
        initial_epoch = int(f.read().strip())
    print(f"Resuming training from epoch {initial_epoch + 1}")
else:
    initial_epoch = 0
    print("Starting training from epoch 1")

def save_last_epoch(epoch):
    with open(last_epoch_path, 'w') as f:
        f.write(str(epoch + 1))

early_stopping = EarlyStopping(
    monitor='val_loss',
    patience=3,
    restore_best_weights=True
)

total_images = sum(len(files) for _, _, files in os.walk(DATASET_PATH) if files)
steps_per_epoch = total_images // BATCH_SIZE
validation_steps = total_images // (BATCH_SIZE * 5)

checkpoint_callback = ModelCheckpoint(
    filepath=checkpoint_path,
    save_weights_only=False,
    save_best_only=False,
    monitor='val_loss',
    mode='min',
    verbose=1
)

print("Training the model...")
history = model.fit(
    train_generator,
    steps_per_epoch=steps_per_epoch,
    validation_data=validation_generator,
    validation_steps=validation_steps,
    epochs=10,
    initial_epoch=initial_epoch,
    callbacks=[
        checkpoint_callback,
        early_stopping,
        keras.callbacks.LambdaCallback(on_epoch_end=lambda epoch, logs: save_last_epoch(epoch))
    ]
)

history_path = os.path.join(SAVE_DIR, 'training_history.json')
with open(history_path, 'w') as f:
    json.dump(history.history, f)
print(f"Training history saved to: {history_path}")

print("Evaluating the model...")
test_loss, test_accuracy = model.evaluate(validation_generator, steps=validation_steps)
print(f"Test Accuracy: {test_accuracy * 100:.2f}%")

results_path = os.path.join(SAVE_DIR, 'evaluation_results.json')
with open(results_path, 'w') as f:
    json.dump({'Test Loss': test_loss, 'Test Accuracy': test_accuracy}, f)
print(f"Evaluation results saved to: {results_path}")
