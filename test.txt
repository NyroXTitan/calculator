# Import necessary libraries
import os
import json
import random
import numpy as np
from PIL import Image
from tensorflow.keras.models import Sequential, load_model
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Input, Dropout
from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, LambdaCallback
from tensorflow.keras.regularizers import l2
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.preprocessing.image import ImageDataGenerator
import tensorflow.keras as keras
from tensorflow.keras.optimizers.schedules import ExponentialDecay
from google.colab import drive

# Mount Google Drive to access the dataset and save results
drive.mount('/content/drive')

# Path to save model and results
SAVE_DIR = '/content/drive/MyDrive/saved_model_results/'  # Set the directory where model and results will be stored
os.makedirs(SAVE_DIR, exist_ok=True)  # Create the directory if it doesn't exist

# Path to save the last completed epoch
last_epoch_path = os.path.join(SAVE_DIR, 'last_epoch.txt')

# Path to dataset
EXTRACTED_PATH = "/content/sample_data/my_class/by_class"  # Path where dataset is stored
DATASET_PATH = EXTRACTED_PATH  # Dataset path for training and validation

# Custom deterministic generator to load images sequentially
def custom_image_generator(dataset_path, batch_size, image_size=(28, 28), shuffle=True, seed=42):
    random.seed(seed)  # Set a fixed random seed for reproducibility (to ensure the shuffle order is the same each time)
    class_folders = [os.path.join(dataset_path, label) for label in os.listdir(dataset_path)]  # Get all subdirectories (class folders)
    class_labels = {label: idx for idx, label in enumerate(os.listdir(dataset_path))}  # Assign numeric labels to each class folder

    # Load all image paths from the dataset
    all_images = []
    for class_folder in class_folders:
        img_files = [
            os.path.join(root, file)
            for root, _, files in os.walk(class_folder)  # Walk through all subfolders and files
            for file in files if file.endswith(".png")  # Only consider .png images
        ]
        all_images.extend((img, class_labels[os.path.basename(class_folder)]) for img in img_files)  # Store image paths and their corresponding labels

    # Generator logic to yield batches of images and labels
    while True:
        if shuffle:
            random.shuffle(all_images)  # Shuffle the images for each epoch to improve training generalization
        for i in range(0, len(all_images), batch_size):  # Process the images in batches
            batch_images, batch_labels = [], []  # Initialize empty lists for images and labels in the current batch
            batch_data = all_images[i:i + batch_size]  # Get the current batch

            for img_path, label in batch_data:
                img = Image.open(img_path).convert('L').resize(image_size)  # Load and convert the image to grayscale, then resize it
                batch_images.append(np.array(img) / 255.0)  # Normalize pixel values to [0, 1]
                batch_labels.append(label)  # Append the label

            # Yield the batch of images and labels
            yield np.expand_dims(np.array(batch_images), -1), to_categorical(batch_labels, num_classes=len(class_labels))

# Data augmentation setup using ImageDataGenerator
datagen = ImageDataGenerator(
    rotation_range=30,  # Random rotation of images between -30 and 30 degrees
    width_shift_range=0.2,  # Random horizontal shift of images (20% of the image width)
    height_shift_range=0.2,  # Random vertical shift of images (20% of the image height)
    shear_range=0.2,  # Random shear transformation (shear angle)
    zoom_range=0.2,  # Random zoom of images (20% zoom)
    horizontal_flip=False,  # Do not apply horizontal flip (not useful for handwriting data)
    fill_mode='nearest'  # Fill pixels after transformation with nearest value
)

# Generator setup using flow_from_directory for training images
BATCH_SIZE = 32
train_generator = datagen.flow_from_directory(
    DATASET_PATH,  # Path to the dataset
    target_size=(28, 28),  # Resize all images to 28x28 pixels
    batch_size=BATCH_SIZE,  # Set batch size
    class_mode='categorical',  # Multi-class classification (one-hot encoding)
    color_mode='grayscale'  # Load images in grayscale
)

# Generator setup for validation images (same as training)
validation_generator = datagen.flow_from_directory(
    DATASET_PATH,
    target_size=(28, 28),
    batch_size=BATCH_SIZE,
    class_mode='categorical',
    color_mode='grayscale'
)

# Model definition with Dropout and L2 Regularization
model = Sequential([
    Input(shape=(28, 28, 1)),  # Input layer (grayscale image with 28x28 pixels)
    Conv2D(32, (3, 3), activation='relu', kernel_regularizer=l2(0.001)),  # Convolutional layer with L2 regularization
    MaxPooling2D((2, 2)),  # Max pooling layer to reduce spatial dimensions
    Conv2D(64, (3, 3), activation='relu', kernel_regularizer=l2(0.001)),  # Second convolutional layer with L2 regularization
    MaxPooling2D((2, 2)),  # Max pooling layer to reduce spatial dimensions further
    Flatten(),  # Flatten the 2D output to 1D for dense layers
    Dropout(0.7),  # Dropout layer to reduce overfitting by randomly dropping 70% of neurons
    Dense(128, activation='relu'),  # Fully connected layer with 128 neurons
    Dense(len(os.listdir(DATASET_PATH)), activation='softmax')  # Output layer with softmax activation (for multi-class classification)
])

# Compile the model with Adam optimizer and categorical crossentropy loss
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

# Check for existing checkpoint to resume training
checkpoint_path = os.path.join(SAVE_DIR, 'latest_checkpoint.keras')  # Path to save the model checkpoint
if os.path.exists(checkpoint_path):  # If checkpoint exists, load it
    print(f"Checkpoint found. Loading model from {checkpoint_path}")
    model = load_model(checkpoint_path)  # Load the entire model (architecture + weights)
else:  # If no checkpoint, start training from scratch
    print("No checkpoint found. Starting training from scratch.")

# Load the last completed epoch from a file
if os.path.exists(last_epoch_path):  # If file exists, read the last completed epoch
    with open(last_epoch_path, 'r') as f:
        initial_epoch = int(f.read().strip())
    print(f"Resuming training from epoch {initial_epoch + 1}")
else:  # If no previous epoch file, start from the first epoch
    initial_epoch = 0
    print("Starting training from epoch 1")

# Function to save the last completed epoch during training
def save_last_epoch(epoch):
    with open(last_epoch_path, 'w') as f:
        f.write(str(epoch + 1))  # Increment epoch by 1 to reflect the completed epoch

# Early Stopping to prevent overfitting (stop training if validation loss stops improving)
early_stopping = EarlyStopping(
    monitor='val_loss',  # Monitor validation loss
    patience=3,  # Stop training after 3 epochs with no improvement
    restore_best_weights=True  # Restore the model with the best weights (lowest validation loss)
)

# Determine steps per epoch and validation steps based on the dataset size
total_images = sum(len(files) for _, _, files in os.walk(DATASET_PATH) if files)  # Get total number of images
steps_per_epoch = total_images // BATCH_SIZE  # Number of steps per epoch
validation_steps = total_images // (BATCH_SIZE * 5)  # Number of validation steps (using 20% of the data)

# Model Checkpoint to save the model after each epoch
checkpoint_callback = ModelCheckpoint(
    filepath=checkpoint_path,  # Path to save the checkpoint
    save_weights_only=False,  # Save the entire model (not just weights)
    save_best_only=False,  # Save the model after every epoch (not just the best)
    monitor='val_loss',  # Monitor validation loss
    mode='min',  # Save the model with the lowest validation loss
    verbose=1  # Print a message when saving the checkpoint
)

# Train the model using the generator
print("Training the model...")
history = model.fit(
    train_generator,  # Training data generator
    steps_per_epoch=steps_per_epoch,  # Number of steps per epoch
    validation_data=validation_generator,  # Validation data generator
    validation_steps=validation_steps,  # Number of validation steps
    epochs=10,  # Total number of epochs
    initial_epoch=initial_epoch,  # Resume from the last completed epoch
    callbacks=[  # Callbacks to save model and handle early stopping
        checkpoint_callback,
        early_stopping,  # Early stopping callback
        keras.callbacks.LambdaCallback(on_epoch_end=lambda epoch, logs: save_last_epoch(epoch))  # Save last completed epoch
    ]
)

# Save training history to JSON
history_path = os.path.join(SAVE_DIR, 'training_history.json')  # Path to save training history
with open(history_path, 'w') as f:
    json.dump(history.history, f)  # Save the history (accuracy, loss) to a JSON file
print(f"Training history saved to: {history_path}")

# Evaluate the model on the validation data
print("Evaluating the model...")
test_loss, test_accuracy = model.evaluate(validation_generator, steps=validation_steps)  # Evaluate the model
print(f"Test Accuracy: {test_accuracy * 100:.2f}%")  # Print the test accuracy

# Save evaluation results to JSON
results_path = os.path.join(SAVE_DIR, 'evaluation_results.json')  # Path to save evaluation results
with open(results_path, 'w') as f:
    json.dump({'Test Loss': test_loss, 'Test Accuracy': test_accuracy}, f)  # Save results to JSON
print(f"Evaluation results saved to: {results_path}")
